---------------C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN-----------------------
!pip uninstall -y peft transformers bitsandbytes accelerate -q

!pip install -q git+https://github.com/huggingface/transformers.git
!pip install -q git+https://github.com/huggingface/peft.git
!pip install -q git+https://github.com/huggingface/accelerate.git
!pip install -q bitsandbytes

!pip install -q evaluate jiwer ctranslate2 faster-whisper
!pip install -q audiomentations soundfile librosa datasets

print("‚úÖ C√†i ƒë·∫∑t xong")

---------------MOUNT DRIVE + C·∫§U H√åNH-----------------------
from google.colab import drive
drive.mount("/content/drive")

MODEL_NAME = "openai/whisper-small"
LANGUAGE = "Vietnamese"
TASK = "transcribe"

DATASET_DIR = "/content/drive/MyDrive/smart_home_dataset"
OUTPUT_DIR  = "/content/drive/MyDrive/whisper_lora_pipeline"

---------------MLOAD DATASET + CHU·∫®N H√ìA AUDIO-----------------------
from datasets import load_dataset, Audio
from transformers import WhisperProcessor

dataset = load_dataset("audiofolder", data_dir=DATASET_DIR)

if "test" not in dataset:
    dataset = dataset["train"].train_test_split(test_size=0.1)

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

processor = WhisperProcessor.from_pretrained(
    MODEL_NAME,
    language=LANGUAGE,
    task=TASK
)

---------------KHAI B√ÅO DATA AUGMENTATION-----------------------
from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Gain

augmentator = Compose([
    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),
    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),
    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),
    Gain(p=0.5),
])

print("‚úÖ Data Augmentation s·∫µn s√†ng")

---------------PREPROCESS + AUGMENT + TOKENIZE-----------------------
!pip install -q torchcodec
print("‚úÖ ƒê√£ c√†i torchcodec ‚Äì h√£y RESTART runtime")

def prepare_dataset(batch):
    audio = batch["audio"]["array"]

    # üîπ G·ªòP: x·ª≠ l√Ω + bi·∫øn ƒë·ªïi √¢m thanh
    audio_aug = augmentator(samples=audio, sample_rate=16000)

    # üîπ Whisper feature
    batch["input_features"] = processor.feature_extractor(
        audio_aug,
        sampling_rate=16000
    ).input_features[0]

    batch["labels"] = processor.tokenizer(
        batch["sentence"]
    ).input_ids

    return batch

dataset = dataset.map(
    prepare_dataset,
    remove_columns=dataset["train"].column_names,
    num_proc=1
)

print("‚úÖ Dataset ƒë√£ ƒë∆∞·ª£c preprocess + augment + tokenize")

--------------------DATA COLLATOR-----------------------
import torch
from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features):
        inputs = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(inputs, return_tensors="pt")

        labels = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(labels, return_tensors="pt")

        batch["labels"] = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor)

-------------------------LOAD WHISPER + LORA (INT8)--------------------------------
from transformers import WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(load_in_8bit=True)

model = WhisperForConditionalGeneration.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    use_cache=False
)

model = prepare_model_for_kbit_training(model)

lora_config = LoraConfig(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

------------------TRANNING-------------------------------------
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    learning_rate=1e-4,
    max_steps=500,
    fp16=True,
    logging_steps=25,
    save_steps=100,
    remove_unused_columns=False,
    report_to="tensorboard"
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    data_collator=data_collator
)

trainer.train()

model.save_pretrained(f"{OUTPUT_DIR}/lora")
processor.save_pretrained(f"{OUTPUT_DIR}/lora")

print("‚úÖ Train LoRA xong")